{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"如何得到一个空的Hadoop Configuration","text":"都怪JDK， File类的delete()方法竟然不能删除非空的目录。所以，我就想用Hadoop的API，于是有了下面一段程序 1234567val fileSystem = FileSystem.newInstance(new Configuration())val warehousePath = new Path(\"spark-warehouse\")if(fileSystem.exists(warehousePath)) fileSystem.delete(warehousePath, true)val metastoreDB = new Path(\"metastore_db\")if (fileSystem.exists(metastoreDB)) fileSystem.delete(metastoreDB) 问题是，new Configuration()默认会从classpath里找到core-site.xml和core-default.xml来加载，所以我想，万一以后不小心把这些文件加到classpath里呢？比如哪天我想要测试连接别的机器上的HDFS。还好，Configuration类有个方法来禁止对这俩文件的加载， 正如这个类的注释所说的 1234Unless explicitly turned off, Hadoop by default specifies two resources, loaded in-order from the classpath:1. `core-default.xml`: Read-only defaults for hadoop.2. `core-site.xml`: Site-specific configuration for a given hadoop installation. 我搞了个core-site.xml到classpath下，于是这段代码就会报错说 in thread \"main\" java.lang.IllegalArgumentException: java.net.UnknownHostException: cdh12345678910111213141516 at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378) at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:320) at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176) at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:678) at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:619) at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703) at org.apache.hadoop.fs.FileSystem$Cache.getUnique(FileSystem.java:2691) at org.apache.hadoop.fs.FileSystem.newInstance(FileSystem.java:420) at org.apache.hadoop.fs.FileSystem.newInstance(FileSystem.java:428) at sleepy.spark.SparkHiveExample$.main(SparkHiveExample.scala:45) at sleepy.spark.SparkHiveExample.main(SparkHiveExample.scala)Caused by: java.net.UnknownHostException: cdh ... 14 more 看起来程序去连接外部的HDFS时，发现无法识别cdh， 实际上它也并非是域名，而是dfs.nameservices的值。 好的，那就用Configuration(boolean)这个构造器, 这的文档是这样说的 A new configuration where the behavior of reading from the default resources can be turned off. If the parameter loadDefaults is false, the new instance will not load resources from the default files. 但是呢，执行的时候仍然在报错 12345678910111213141516171819202122232425262728293031323334Exception in thread &quot;main&quot; java.io.IOException: failure to login at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:841) at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650) at org.apache.hadoop.fs.FileSystem$Cache$Key.&lt;init&gt;(FileSystem.java:2828) at org.apache.hadoop.fs.FileSystem$Cache.getUnique(FileSystem.java:2690) at org.apache.hadoop.fs.FileSystem.newInstance(FileSystem.java:420) at org.apache.hadoop.fs.FileSystem.newInstance(FileSystem.java:428) at sleepy.spark.SparkHiveExample$.main(SparkHiveExample.scala:47) at sleepy.spark.SparkHiveExample.main(SparkHiveExample.scala)Caused by: javax.security.auth.login.LoginException: java.lang.IllegalArgumentException: Illegal principal name foo@FOO.COM: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to foo@FOO.COM at org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:201) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) at javax.security.auth.login.LoginContext.login(LoginContext.java:588) at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:815) ... 8 moreCaused by: java.lang.IllegalArgumentException: Illegal principal name foo@FOO: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to foo@FOO at org.apache.hadoop.security.User.&lt;init&gt;(User.java:51) at org.apache.hadoop.security.User.&lt;init&gt;(User.java:43) at org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:199) ... 20 moreCaused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to foo@FOO at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:400) at org.apache.hadoop.security.User.&lt;init&gt;(User.java:48) ... 22 more 究其原因是在初始化FileSystem的时候，化调用到UserGroupInformation#ensureIntialized() 123456789private static void ensureInitialized() { if (conf == null) { synchronized(UserGroupInformation.class) { if (conf == null) { // someone might have beat us initialize(new Configuration(), false); } } }} 这里直接调用了new Configuration, 而这个对象是加载了classpath里的配置文件的。 所以，直接给UGI指定个configuration就行了 123val conf = new Configuration(false)UserGroupInformation.setConfiguration(conf)val fileSystem = FileSystem.newInstance(conf)","link":"/2020/07/11/hadoop-empty-configuration/"},{"title":"mio::poll文档 —— 翻译和注解","text":"翻译自mio文档: Poll Struct mio::Poll1pub struct Poll { /* fields omitted */ } Polls for readiness events on all registered values. Poll allows a program to monitor a large number of Evented types, waiting until one or more become “ready” for some class of operations; e.g. reading and writing. An Evented type is considered ready if it is possible to immediately perform a corresponding operation; e.g. read or write. To use Poll, an Evented type must first be registered with the Poll instance using the register method, supplying readiness interest. The readiness interest tells Poll which specific operations on the handle to monitor for readiness. A Token is also passed to the register function. When Poll returns a readiness event, it will include this token. This associates the event with the Evented handle that generated the event. Poll用于从所有注册的value里poll处于就绪状态的事件。这个跟Java里NIO里的概念可以类比下： Selector &lt;-&gt; Poll。可以把Evented注册到Poll上，使用它来监听Evented有关的IO事件。是实现异步IO的关键组件。 Evented &lt;-&gt; SelectionKey。可以通过SelectionKey获取底层的channel进行读写。Evented可以直接进行读写。 Token &lt;-&gt; SelectionKey里attach的对象。用于将事件跟Evented关联起来。 例子 一个基础的例子，创建一个TcpStream连接。 1234567891011121314151617181920212223242526272829303132333435use mio::{Events, Poll, Ready, PollOpt, Token};use mio::net::TcpStream;use std::net::{TcpListener, SocketAddr};// 创建一个server socket, 绑定到指定地址let addr: SocketAddr = \"127.0.0.1:0\".parse()?;let server = TcpListener::bind(&amp;addr)?;// Construct a new `Poll` handle as well as the `Events` we'll store intolet poll = Poll::new()?;let mut events = Events::with_capacity(1024);// Connect the streamlet stream = TcpStream::connect(&amp;server.local_addr()?)?;// 把stream注册到`Poll`poll.register(&amp;stream, Token(0), Ready::readable() | Ready::writable(), PollOpt::edge())?;// Wait for the socket to become ready. This has to happens in a loop to// handle spurious wakeups.//等待socket可用。需要在一个处理虚假的wakeup的循环里做这件事loop { poll.poll(&amp;mut events, None)?; for event in &amp;events { if event.token() == Token(0) &amp;&amp; event.readiness().is_writable() { // The socket connected (probably, it could still be a spurious // wakeup) // socket连接上了(只是可能连接上了，仍然可能是一个假的wakeup) return Ok(()); } }} Events::with_capacity在poll的源码的注释里写到 1234/// The supplied `events` will be cleared and newly received readiness events/// will be pushed onto the end. At most `events.capacity()` events will be/// returned. If there are further pending readiness events, they will be/// returned on the next call to `poll`. Edge-triggered and level-triggered 问题在于，对于已经处于就绪状态的key，如果我们在poll返回的集合里没有对它进行操作, 例如没有读写，那么下一次poll的时候，它还会在集合里吗？ Java NIO 的作法Java NIO的Selector维持了三个集合: key set。包含了所有注册到selector的channel对应的key selected-key set。在这个集合中的key对应的channel处于readiness状态。 cancelled-key set。这些key已经被cancel，但是相关的channel还没有被deregistered。 Keys are added to the selected-key set by selection operations. A key may be removed directly from the selected-key set by invoking the set’s remove method or by invoking the remove method of an iterator obtained from the set. Keys are never removed from the selected-key set in any other way; they are not, in particular, removed as a side effect of selection operations. Keys may not be added directly to the selected-key set. 对Java的NIO框架，所有处于就绪状态的key会一直在Selector#selectedKey()返回的集合中，除非你主动移除它。 Rust mio的作法Rust对上边这个问题的处理，给用户提供了两种选择：edge-triggered 以及 level-triggered.这个跟电路的相关知识类似。下边是对相关文档的解要翻译： 一个Evented在注册的时候可以选择请求的是edge-triggered事件，还是level-triggered的事件。试想有下面的场景发生： 通过Poll注册了一个TcpStream socket接收到了2kb数据 对Poll::poll的调用返回了token以及绑定在这个token上的socket，指示说这个socket处于读就绪状态。 从socket里读取了1kb数据 再次调用Poll::poll 如果在注册这个socket时要求的是edge-triggered event，那么当在第5步调用Poll::poll时可能会hang住，即使现在在socket的read buffer里有1kb数据。原因是edge-triggered模式只有当被监听的Evented有事件发生时才会投递事件。 当使用edger-triggered events时，必须对Evented进行处理，直接它返回WouldBlock。按句话说，在收到了指标说就绪状态的消息以后，你必须假设Poll::poll以后不会对这个token再返回消息，直到你对它的操作返回WouldBlock。 作为对比的是，当要求的是level-triggered通知时，只要相关的socket buffer有数据就会返回event。通常来说，当需要高性能时，就要避免使用level-triggered events. Since even with edge-triggered events, multiple events can be generated upon receipt of multiple chunks of data, the caller has the option to set the oneshot flag. This tells Poll to disable the associated Evented after the event is returned from Poll::poll. The subsequent calls to Poll::poll will no longer include events for Evented handles that are disabled even if the readiness state changes. The handle can be re-enabled by calling reregister. When handles are disabled, internal resources used to monitor the handle are maintained until the handle is dropped or deregistered. This makes re-registering the handle a fast operation. For example, in the following scenario: A TcpStream is registered with Poll. The socket receives 2kb of data. A call to Poll::poll returns the token associated with the socket indicating readable readiness. 2kb is read from the socket. Another call to read is issued and WouldBlock is returned The socket receives another 2kb of data. Another call to Poll::poll is made.Assuming the socket was registered with Poll with the edge and oneshot options, then the call to Poll::poll in step 7 would block. This is because, oneshot tells Poll to disable events for the socket after returning an event. In order to receive the event for the data received in step 6, the socket would need to be reregistered using reregister. 这一段是说在注册到Poll的时候有个oneshot标识，当它被set以后，当相关的Evented的第一个event被返回以后，这个Evented的handle就被disable了，直到你调用reregister。由于在Evented的handle被disable以后，监控它所使用的内部资源没有被释放，所以这个re-register操作是一个很快的操作。 这个特性是Java所没有的。 可移植性 只要按照下面的规范，Poll就提供了对所支持的所有平台都可用的接口。 Spurious Events即使相关联的Eventedhandle没有真的就绪，Poll::poll也可能返回就绪事件。 如果操作失败，返回WouldBlock，那么调用者不应该把它当作一个错误，因该等待下一个就绪事件到来。 Draining readiness当使用edge-triggered模式时，当收到一个就绪事件时，应该一直执行操作，直到WouldBlock被返回。 Readiness operations只有readable和writable才是所有平台都支持的readiness operation。所以即使注册了对hup和error的关注，当收到相关消息时，也只在通过实际的读写才能知道直正的情况。 Registering handles除非另外说明，应该假设实现了Evented的类型只有在被注册到Poll以后才可能就绪。","link":"/2018/06/07/mio_poll/"},{"title":"关于reborrow的一个复杂的例子","text":"在查找关于Rust的reborrow的语法时，发现这么一篇文章Stuff the Identity Function Does (in Rust)。然后……看不懂，《Programming Rust》快看完了，这篇文章还是看不懂。但是有很多不懂之处的文章，往往是最值得读的，因为它提供了一个线索，能把遗漏的知识串连起来，这是很难得的。 还好有Google, 一路搜索过来，大体也搞清楚了。 例子文章里例子是这样的。有一个递归的数据结构，List: 123struct List { next: Option&lt;Box&lt;List&gt;&gt;,} 写一个函数来遍历它 1234567891011impl List { fn walk_the_list(&amp;mut self) { let mut current = self; loop { match current.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } } }} 可以在Rust的playgroud里测试一下。你会发现这段代码是通不过编译的。问题在哪呢？ 实际上这短短一段代码使用了很多隐晦的语法。 先来搞清楚它在做什么吧。 match的是什么？在Rust的match表达式的分支里，是可以用&amp;来匹配reference的，比如： 123456789struct Foo { v: i32}let mut a = Foo{v: 1};match &amp;a { &amp;Foo{v} =&gt; println!(\"{}\", v + 1), _ =&gt; panic!()} (注： Rust 1.26里有了更简化的写法，见Announcing Rust 1.26里的”Nicer match bindings“) 但是在List的例子里， 不是这种情况， current.next并非一个reference。所以，这个match表达式绝非在match一个reference，而是一个类型为&lt;Option&lt;Box&lt;List&gt;&gt;的值。 ref 关键字some(ref mut inner)这句以前貌似没见过。查了一下，发现这是在match表达式的分支里专用的一个语法。可以参照这篇文章:&amp; vs. ref in Rust patterns。ref用于把一个原来需要move的地方，改成只获取reference，从而避免move。在前边的代码里，实际上就是想避免对current.next的move。实际上，由于current是一个reference，是不能通过它把current.next的值move out出去的。 比如, 下边的代码试着把current.next move给d: 1234fn walk_the_list(mut self) { let mut current = &amp;mut self; let d = current.next;} 编译器会告诉你 12345678error[E0507]: cannot move out of borrowed content --&gt; src/main.rs:54:21 | 54 | let d = current.next; | ^^^^^^^----- | | | cannot move out of borrowed content | help: consider using a reference instead: `&amp;current.next` 错误信息和一些细节如果你在试着编译最上边关于List的代码(也可以playgroud里执行run)，编译器会报两个error。 12345678910111213141516error[E0499]: cannot borrow `current.next.0` as mutable more than once at a time --&gt; src/main.rs:19:26 |19 | Some(ref mut inner) =&gt; current = inner, | ^^^^^^^^^^^^^ mutable borrow starts here in previous iteration of loop...22 | } | - mutable borrow ends hereerror[E0506]: cannot assign to `current` because it is borrowed --&gt; src/main.rs:19:44 |19 | Some(ref mut inner) =&gt; current = inner, | ------------- ^^^^^^^^^^^^^^^ assignment to borrowed `current` occurs here | | | borrow of `current` occurs here 下边来搞清楚这两个错是怎么造成的。首先明确一下move和borrow这两个概念是否可用于reference，以及(如果可用的话)有什么作用。 move和borrow首先要搞清楚被move和borrow的是什么？move会造成两个结果，一个是被move的值之前所在的内存变成了uninitialized状态，二是把之前的值拷贝到了move的目标内存。所以，不仅像let a = 1里边的a这样的非reference的值可以被move, reference本身也可以被move。像下边这个例子: 12345678struct Foo { v: i32}let mut a = Foo{v:1};let b = &amp;mut a;let c = b;b; 编译器会说use of moved value: b，这是说b作为一个reference，它的值被move了，b成了未初始化状态，编译器会确保后边对b的使用(除非再给b赋值)会报错。 那么，reference可以被borrow吗？也是可以的： 12345678struct Foo { v: i32}let mut a = Foo{v:1};let mut b = &amp;mut a;let c = &amp;mut b;let d = &amp;mut b; 这一段编译器报的错是： cannot borrow b as mutable more than once at a time 这就跟List的例子里编译器报的错很相似了。 cannot borrow current.next.0 as mutable more than once at a time DerefMut这里说同时不能borrow current.next.0超过一次。这个borrow是发生在Some(ref mut inner)这个branch里。但是current.next.0是个什么鬼？而且current = inner这两边的类型不一致呀。 inner的类型是&amp;mut Box&lt;List&gt;， 而current的类型是&amp;mut List。这里是有一个隐式的转换的，把&amp;mut Box&lt;List&gt;转成了&amp;mut List。这应该是Box实现了Defmut这个trait。实际上也是如此，Box实现了这个trait 1impl&lt;T: ?Sized&gt; DerefMut for Box&lt;T&gt; 而DerefMut的定义是这样的 123trait DerefMut: Deref { fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target;} 所以，由于需一个&amp;mut List，但是目前只有&amp;mut Box&lt;List&gt;。所以，Rust编译器会调用Box::deref_mut(inner)，返回对Box内的List的mutable reference。好了，我们了解了inner的类型为&amp;mut Box&lt;List&gt;，它是对next为Some时其中的Box&lt;List&gt;的mutable reference。它被赋值给current时发生了地deref_mut的调用(一个隐式类型转换)。 current是怎么被borrow的？前边已知道的是，current.next.0是被borrow的状态。那么，为什么编译器同时也认为current也是在被borrow状态呢？而且，在z = &amp;mut current.next的例子中，borrow的是current.next, 编译器也是报错说current被borrow了多次。 auto-dereferencecurrent.next很明显是说List里的叫next的field。但是current是一个reference呀，它没有next这个field呀。这里是用的Rust的auto-dereference语法，实际上是调用的(*current).next。但是，还不只这么简单，看下边的例子。 通过current.next进行的reborrow12345fn walk_the_list(&amp;mut self) { let mut current = self; let d = &amp;mut current.next; let z = &amp;mut current;} 这里编译器会报错说： error[E0499]: cannot borrow current as mutable more than once at a time –&gt; src/main.rs:37:26 | 36 | let d = &amp;mut current.next; | ———— first mutable borrow occurs here 37 | let z = &amp;mut current; | ^^^^^^^ second mutable borrow occurs here 38 | } | - first borrow ends here 它不会允许我们再次borrow current这个reference了，因为我们通过&amp;mut current.next隐式地使得current被borrow了。注意，这是使得current这个reference被borrow了，而非current指向的值被borrow了。 不过，事实上*current也被borrow了。看这个例子 12345fn walk_the_list(&amp;mut self, other: List) { let mut current = self; let b = &amp;mut current.next; *current = other;} 编译器会说: 1234567error[E0506]: cannot assign to `*current` because it is borrowed --&gt; src/main.rs:47:13 |46 | let d = &amp;mut current.next; | ------------ borrow of `*current` occurs here47 | *current = other; | ^^^^^^^^^^^^^^^^ assignment to borrowed `*current` occurs here 当然，current就是*current的mutable reference。但是，编译器的这句话指向的是在let d = &amp;mut current.next时， *current被borrow的。这句话应该这么看，如果只有current这唯一的一个mutable reference，那么*current = other就是合理的。但是，let d = &amp;mut current.next产生了另一个我们拿不到的mutable reference，可以把&amp;mut a.b可以认为实际上执行的是&amp;mut (*a).b, 这样*a就被borrow了，这样在d的生命周期里，current就不可用了。 也就是说，如果我们通过let c = &amp;mut a.b的形式来搞到一个b的mutable reference时。编译器会使得a处于mutable borrowed状态, 无论a是不是reference。而且假如b的owner path的更上游也会处于这个状态。 ownship path关于reference的 ‘shared vs mutation’,《Programming Rust》里这么说： Each kind of reference affects what we can do with the values along the owning path to the reference, and the values reachable from the reference.Note that in both cases, the path of ownership leading to the referent cannot be changed for the references’s lifetime.For a shared borrow, the path is readonly; for a mutable borrow, it’s completely inaccessible. So there’s no way for the program to do anything that will invalidate the reference. Rust这样做的目的，就是使刚才获得的mutable reference:c不会成为dangling pointer。当我们获取一个mutable reference时，例如inner, 它是对current.next.0的mutable borrow， 那么 the path of ownship leading to current.next.0 在inner的lifetime中都是’completely inaccessible’的。这里 inaccessible 并非是说在以后的代码里就不能用current这个变量了，而是说不通单独使用它，例如let x = current。而 the path of ownship leading to the referent 是说inner所指向的值，也就是current.next.0的owner, 以及owner的owner … 。由于当一个值有了mutable reference期间，只能通过这个reference使用它，所以可以认为mutable reference在这个ownship path上跟它的referent是处于同一位置。所以current也就成了 inaccessible 的了。 loop现在还是没有搞清楚为什么这个loop block会导致错误。 可以参见borrowing in loops, 首先，假如原代码改成 123456789fn walk_the_list(&amp;mut self) { let mut current = self; loop { match current.next { None =&gt; return, Some(ref mut inner) =&gt; (), } }} 就可以通过编译了。显然，是current = inner这一句导致了在loop的一次迭代结束后，之前的current.next.0仍然处于mutable borrowed状态。而去掉current = inner以后，inner作为对current.next.0的borrow的生命周期结束了，同时使得对current的隐式的borrow结束了。比如，下边这段代码会报错: 12345678fn walk_the_list(&amp;mut self) { let mut current = self; match current.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } let ref_current = &amp;mut current.next;} 编译器说 12345678910error[E0499]: cannot borrow `current.next` as mutable more than once at a time --&gt; src/main.rs:65:36 |63 | Some(ref mut inner) =&gt; current = inner, | ------------- first mutable borrow occurs here64 | }65 | let ref_current = &amp;mut current.next; | ^^^^^^^^^^^^ second mutable borrow occurs here66 | } | - first borrow ends here 虽然current = inner这句不能通过编译，但是编译器仍然认为inner的生命周期在match结束后没有结束，这就使得current.next仍然处于被borrow的状态。在原来的例子里，这就意味着current.next.0仍然处于被borrow的状态。同样由于current = inner不能执行，编译器认为两次迭代borrow的是同一个current.next.0。 你需要的只是move那么怎么可以让它摆脱这种状态呢？编译器一直报怨的是多次borrow了current这个reference它的值。所以只要在match前把它move出来就行了。因为当current再次被赋值时，就可以认为对它的borrow跟对上一个current的borrow完全没关系了，只不过被borrow的值的名字都叫current而已。这也是move这个语法的本意，一个变量的值被move走之后，它就成了未初始化状态，就跟之前的值和之前的ownership path啥的没有关系了。当它再被赋值后，它的属性就继承了对刚才被赋的值的属性，跟它的前世也没啥关系。 下边我们用上作者提到的id函数 1234567891011121314fn id&lt;T&gt;(x: T) -&gt; T { x }fn walk_the_list(&amp;mut self) { let mut current = self; match id(current).next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } match current.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, }} 这次编译器就不会再报多次borrow的错误了。因为id函数把current的值move给了函数参数。 这里要注意id的实现细节，它是一个generic function，如果参数的类型不是T，而是&amp;mut List，即把id改成： 1fn id(x: &amp;mut List) -&gt; &amp;mut List { x } 就还会有同样的错误。这就牵扯到了一个更隐晦的问题，就是Rust什么时候会move，什么时候会reborrow。在Why can I use an &amp;mut reference twice?里有所提及。其中的一个说法是，&amp;Mut List并非current的全部类型，因为所有的reference的lifetime是它的类型的一部分，所以，当把current传给修改后的id时，编译器会使得reborrow，而非move。但是使用泛型，就可以完全匹配current的类型。 事实上，还有别的方法可以通过编译，而且不用id函数，例如: 12345678910fn walk_the_list(&amp;mut self) { let mut current = self; loop { let mut tmp = current; match tmp.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } }} 这里显式地把current给 move到了z，使得后边的current = innner可以成功。编译器就可以进行同样的推导。 而且，正如原文中提到的，可以用Rust的一种特殊的语法 – {}, 来进行move。 123456789fn walk_the_list_with_braces(&amp;mut self) { let mut current = self; loop { match {current}.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } }} 现在想一下这段代码的作用，它只是walk the list， 在方法执行完以后，这个List还跟原来一样。所以，如果只是实现同样的功能，下边的代码就够了： 123456789fn walk_the_list(&amp;self) { let mut current = self; loop { match current.next { None =&gt; return, Some(ref inner) =&gt; current = inner, } }} 原文在最后提了一下consume这个list的方法 123456789101112131415impl List { fn walk_the_list_with_braces(&amp;mut self) { let mut current = self; loop { match {current}.next { None =&gt; return, Some(ref mut inner) =&gt; current = inner, } } } fn consume_the_list_with_braces(self) { {self}.walk_the_list_with_braces(); }} 这里在consume_the_list_with_braes里{self}貌似没有必要，但是如果你去掉{}，编译器会说： 123456789error[E0596]: cannot borrow immutable argument `self` as mutable --&gt; src/main.rs:45:13 |44 | fn consume_the_list_with_braces(self) { | ---- consider changing this to `mut self`45 | self.walk_the_list_with_braces(); | ^^^^ cannot borrow mutablyerror: aborting due to previous error 由于是self而非mut self，self是一个immutable binding, 是不能对它进行 mutable borrow的。但是通过{}，self的值被move出来到了一个临时的值中，没有对它的任何immutable binding，这样就绕过了immutable binding的限制。","link":"/2018/05/17/Rust_a_complex_example/"},{"title":"spark里的filter和projection pushdown","text":"1. 概述predicate pushdown也叫filter pushdown。 所以，有两种pushdown。 projection pushdown， 用于select 的pushdown filter pushdown, 用于filter的pushdown 先来明确一下这俩在干嘛。 1.1 projection pushdown Projection Pushdown minimizes data transfer between MapR Database and the Apache Spark engine by omitting unnecessary fields from table scans. It is especially beneficial when a table contains many columns. projection pushdown通过在table scan过程中忽略不需要的列来减少从数据源读取的数据量。把projection下堆到数据源处。 当使用select的时候，会进行projection pushdown，比如 1234from pyspark.sql import SparkSession df = spark_session.loadFromMapRDB(\"/tmp/user_profiles\")df.select(\"_id\", \"first_name\", \"last_name\") 1.2 filter pushdown把筛选行的filter下推到数据源处。 也是会减少从数据源传输到spark engine的数据量，但减少的单位是“行”，而projection pushdown减少的单位是“列”。 比如 1234from pyspark.sql import SparkSession df = spark_session.loadFromMapRDB(\"/tmp/user_profiles\")df.filter(\"first_name = 'Bill'\") 支持以下filter的pushdown: = 和!= &lt;, &gt;, &gt;=, &lt;= IN LIKE AND， OR NOT 1.3 限制filter pushdown不支持复杂类型：array, map, struct, 比如 scala 1df.filter($\"address.city\" === \"Milpitas\") java 1df.filter(col(&quot;address.city&quot;).equalTo(&quot;Milpitas&quot;)); projection pushdown也不支持这些复杂类型， 比如 scala 1ds.select($&quot;hobbies&quot; (0)) java 1df.select(col(\"hobbies\").getItem(0)); 但是spark3.0进行了一些改进。 2. databricks关于FilterPushdown的例子databricks有一些例子How logical plan optimizations work in Catalyst 2.1 more interesting example创建两个DataFrame 12345val items = Seq((0, \"Macbook Pro\", 1999.0), (1, \"Macbook Air\", 1500.0), (2, \"iPad Air\", 1200.0)).toDF(\"id\", \"name\", \"price\")val orders = Seq((100, 0, 1), (100, 1, 1), (101, 2, 3)).toDF(\"id\", \"itemid\", \"count\")items.createOrReplaceTempView(\"item\")orders.createOrReplaceTempView(\"order\") 然后搞一个简单的join以及filter 1234SELECT order.id, item.name, item.price, order.countFROM itemJOIN order WHERE item.id = order.itemid and item.price &lt; 1400 and order.count &gt; 2 - 1 2.1.1 analyzed plan然后看analyzed plan 1val analyzedPlan = sql(\"SELECT order.id, item.name, item.price, order.count FROM item JOIN order WHERE item.id = order.itemid and item.price &lt; 1400 and order.count &gt; 2 - 1\").queryExecution.analyzed 结果为 12345678910analyzedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Project [id#15942, name#15929, price#15930, count#15944]+- Filter (((id#15928 = itemid#15943) &amp;&amp; (price#15930 &lt; cast(1400 as double))) &amp;&amp; (count#15944 &gt; (2 - 1))) +- Join Inner :- SubqueryAlias item : +- Project [_1#15924 AS id#15928, _2#15925 AS name#15929, _3#15926 AS price#15930] : +- LocalRelation [_1#15924, _2#15925, _3#15926] +- SubqueryAlias order +- Project [_1#15938 AS id#15942, _2#15939 AS itemid#15943, _3#15940 AS count#15944] +- LocalRelation [_1#15938, _2#15939, _3#15940] 这里边Filter是Join的父节点，意味着filter条件里的item.price &lt; 1400 and order.count &gt; 2 - 1是在join之后才被执行。 2.1.2 optimized plan看optimized plan 12// Apply Spark SQL optimizationsval optimizedPlan = SimpleTestOptimizer.execute(analyzedPlan) 结果为 123456789optimizedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Project [id#15942, name#15929, price#15930, count#15944]+- Join Inner, (id#15928 = itemid#15943) :- Project [_1#15924 AS id#15928, _2#15925 AS name#15929, _3#15926 AS price#15930] : +- Filter (_3#15926 &lt; 1400.0) : +- LocalRelation [_1#15924, _2#15925, _3#15926] +- Project [_1#15938 AS id#15942, _2#15939 AS itemid#15943, _3#15940 AS count#15944] +- Filter (_3#15940 &gt; 1) +- LocalRelation [_1#15938, _2#15939, _3#15940] 这里可以看到几点变化： Filter被下推，直接作用于LocalRelation。 适用的规则为PushDownPredicate 2 - 1 被替换成了1。 适用的规则为ConstantFolding SubqueryAlia节点被直接优化掉了 cast(1400 as double)被换成了1400.0 2.1.3 Write rules for logical plan Catalyst operates with logical plans and expressions, even attribute is an expression. Below are some examples how to convert one expression into another another using rules. Each logical plan is essentially a bunch of QueryPlan[LogicalPlan] instances and Expression expressions. Some examples of logical plans are Project, Generate, Filter, etc. 翻译一下： Catalyst操作的对象是_logical plans_以及_expressions_， 即使是attribute也是一种expression。下边是一些使用规则(rule)将一个表达式转换成另一个的例子。每个logical plan本质是就是一组QueryPlan[LogicalPlan]实例和Expression表达式。logical plan的例子包括Project, Generate, Filter等。 这里有一些东东不大明白： QueryPlan[LogicalPlan]是个什么东东？ plan这个概念和expression有啥区别？ 继续往下看 2.1.3.1 expression的转换12345678// Simple expression transformval add = Add(Literal(2), Literal(3))val subtract = add transform { case Add(left, right) =&gt; Subtract(left, right)}add: org.apache.spark.sql.catalyst.expressions.Add = (2 + 3)subtract: org.apache.spark.sql.catalyst.expressions.Expression = (2 - 3) Analyzer is built from rules, each rule is essentially one operation that takes logical plan and returns logical plan with very minimal change, hopefully better change. See example below: 是说rule就是一个operator，输入和输出都是logical plan， 不过这俩logical plan会有些许不同，目的是进行一些优化。 transform方法的定义为 123def transform(rule: PartialFunction[BaseType, BaseType]): BaseType = { transformDown(rule)} 这里的 123{ case Add(left, right) =&gt; Subtract(left, right)} 就是一个rule。add和subtract就是两个Expression。这里通过一个自己实现的rule，将Add表达式转成了Subtract表达式。 2.1.3.2 TreeNodetransform是Add继承自TreeNode的方法。 这里TreeNode的类型参数有点绕。 123abstract class TreeNode[BaseType &lt;: TreeNode[BaseType]] extends Product {// scalastyle:on self: BaseType =&gt; 在这里要注意两点 TreeNode是一个invariant class，不是协变类，也不是逆变类。 TreeNode要求其自身是一个BaseType。也就是说， BaseType实际上就是指它自己或它的子类。这样在定义方法的时候就可以用到这个限制。比如，TreeNode的foreach方法这么定义 12345678/** * Runs the given function on this node and then recursively on [[children]]. * @param f the function to be applied to each node in the tree. */def foreach(f: BaseType =&gt; Unit): Unit = { f(this) children.foreach(_.foreach(f))} 这里之所以可以f(this), 就是因为self: BaseType =&gt;这个自身类型限制。如果去掉自身类型的限制，那么就f(this)就不合语法了。 实际上它的直接子类都这样类义的： Expression 1abstract class Expression extends TreeNode[Expression] Block 1trait Block extends TreeNode[Block] with JavaCode QueryPlan 12abstract class QueryPlan[PlanType &lt;: QueryPlan[PlanType]] extends TreeNode[PlanType] { self: PlanType =&gt; 这样写的话，Expression继承的TreeNode的方法，如果参数类型是BaseType，那么就只能传进去Expression的子类，而不能传进去Block的子类。 这样是不可以的 123val plan3 = add transform { case foo: LogicalPlan =&gt; foo} 就会报错 pattern type is incompatible with expected type;found : org.apache.spark.sql.catalyst.plans.logical.LogicalPlanrequired: org.apache.spark.sql.catalyst.expressions.Expressioncase foo: LogicalPlan =&gt; foo 也就是说add接受的transform实际上的类型要求是PartialFunction[Expression, Expression] (这里要注意PartitalFunction的型变)。 这么搞，有利于递归调用rule的时候避免错误匹配，比如一个用于Expression的规则被误用到了一个LogicalPlan，就要出问题了。 2.1.4 Existing filter optimizations123456val logicalPlan = LocalRelation(int('a), str('b)). select('a). where(GreaterThan(Add('a, Literal(1)), Literal(2)))val analyzedPlan = logicalPlan.analyzeval optimizedPlan = SimpleTestOptimizer.execute(analyzedPlan) 结果为 1234567891011121314logicalPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = 'Filter (('a + 1) &gt; 2)+- 'Project ['a] +- LocalRelation &lt;empty&gt;, [a#21651, b#21652]analyzedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Filter ((a#21651 + 1) &gt; 2)+- Project [a#21651] +- LocalRelation &lt;empty&gt;, [a#21651, b#21652]optimizedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Project [a#21651]+- Filter (isnotnull(a#21651) &amp;&amp; ((a#21651 + 1) &gt; 2)) +- LocalRelation &lt;empty&gt;, [a#21651, b#21652] 这里可以看到((a#21651 + 1) &gt; 2))并非最优的形式。所以，下面自己创建一个rule来解析这个表达式。 2.1.5 Optimize filter folding创建一个Rule，叫做SimpleFilterFolding。代码为 1234567891011// Let's call our rule SimpleFilterFoldingobject SimpleFilterFolding extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan transform { // We take logical plan and only apply our rule when we encounter filter with a simple `add` condition case filter @ Filter(condition, _) =&gt; filter transformExpressionsUp { // What we need to do is replacing our filter where `expr` is greater than right side - literal case GreaterThan(Add(expr, literal: Literal), right) =&gt; GreaterThan(expr, Subtract(right, literal)) } }} 然后把这个rule注册到我们的optimizer中。 1234567object SimpleOptimizer extends RuleExecutor[LogicalPlan] { val batches = Batch(\"Filter folding\", Once, SimpleFilterFolding) :: Nil}// We take analyzed plan and run through Optimizer objectval optimizedPlan = SimpleOptimizer.execute(analyzedPlan) 结果为 12345defined module SimpleOptimizeroptimizedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Filter (a#21651 &gt; (2 - 1))+- Project [a#21651] +- LocalRelation &lt;empty&gt;, [a#21651, b#21652] 这里(2 - 1)并没有被优化掉。但是已有rule可以做这件事，就是叫做ConstantFolding的rule，现在把它跟我们的rule一起使用。 1234567object SimpleOptimizer extends RuleExecutor[LogicalPlan] { val batches = Batch(\"Filter folding\", Once, SimpleFilterFolding, ConstantFolding) :: Nil}// We take analyzed plan and run through Optimizer objectval optimizedPlan = SimpleOptimizer.execute(analyzedPlan) 这样结果就对了 12345defined module SimpleOptimizeroptimizedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Filter (a#21651 &gt; 1)+- Project [a#21651] +- LocalRelation &lt;empty&gt;, [a#21651, b#21652] Spark 3.0的在filter pushdown方面有了进步，这个下个blog再研究。 参考资料 Projection and Filter Pushdown with Apache Spark DataFrames and Datasets Filter pushdown","link":"/2020/08/16/pushdown/"}],"tags":[{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"Rust","slug":"Rust","link":"/tags/Rust/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"spark","slug":"spark","link":"/tags/spark/"}],"categories":[{"name":"hadoop","slug":"hadoop","link":"/categories/hadoop/"},{"name":"Rust","slug":"Rust","link":"/categories/Rust/"},{"name":"spark","slug":"spark","link":"/categories/spark/"}]}